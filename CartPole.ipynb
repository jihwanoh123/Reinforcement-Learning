{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc1bf8d-686f-4171-a196-d76ca1b4e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Step 1: Environment Setup\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefea39b-7986-47f4-8393-debbd766bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Environment Simulation to collect data\n",
    "def collect_random_data(env, num_episodes=100):\n",
    "    data = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            step_result = env.step(action)\n",
    "            next_obs, reward, done = step_result[:3]\n",
    "            info = step_result[3] if len(step_result) > 3 else {}\n",
    "            data.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "    return data\n",
    "\n",
    "data = collect_random_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c50c26-2c18-4143-9ca8-67d01ff74657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00731177, -0.01372139, -0.00953324,  0.02445259], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, action, reward, next_obs, done=data[0]\n",
    "obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a09023e-819a-413e-986c-3063fd8bdd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.013302328201539043\n",
      "Epoch 2/10, Loss: 0.0023811238704294574\n",
      "Epoch 3/10, Loss: 0.0013715574794935985\n",
      "Epoch 4/10, Loss: 0.0013444001109254057\n",
      "Epoch 5/10, Loss: 0.0008250438280797269\n",
      "Epoch 6/10, Loss: 0.0008949748313825888\n",
      "Epoch 7/10, Loss: 0.0009210502039504032\n",
      "Epoch 8/10, Loss: 0.0007900766803542896\n",
      "Epoch 9/10, Loss: 0.0008016386640197829\n",
      "Epoch 10/10, Loss: 0.0009191531613953992\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Model Learning (Learn the dynamics of the environment)\n",
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(DynamicsModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size + action_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, obs_size + 1)  # Predict next_obs and reward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "dynamics_model = DynamicsModel(obs_size, action_size)\n",
    "optimizer = optim.Adam(dynamics_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def train_dynamics_model(data, model, optimizer, loss_fn, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for obs, action, reward, next_obs, done in data:\n",
    "            obs = np.array(obs[0]) if isinstance(obs, tuple) else np.array(obs)\n",
    "            next_obs = np.array(next_obs[0]) if isinstance(next_obs, tuple) else np.array(next_obs)\n",
    "            action_onehot = np.zeros(action_size)\n",
    "            action_onehot[action] = 1\n",
    "            input_data = np.concatenate([obs, action_onehot])\n",
    "            input_tensor = torch.FloatTensor(input_data).unsqueeze(0)\n",
    "            target_tensor = torch.FloatTensor(np.concatenate([next_obs, [reward]])).unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(input_tensor)\n",
    "            loss = loss_fn(prediction, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {np.mean(losses)}\")\n",
    "\n",
    "train_dynamics_model(data, dynamics_model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9252b92-d153-40eb-9efa-e67d9be78097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihwan/miniforge3/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 5817     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.5        |\n",
      "|    ep_rew_mean          | 25.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4011        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007885848 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.00449    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.96        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 54.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.9        |\n",
      "|    ep_rew_mean          | 33.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3686        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009862224 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0833      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 33.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 46.2         |\n",
      "|    ep_rew_mean          | 46.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3433         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074015213 |\n",
      "|    clip_fraction        | 0.0836       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.637       |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.7         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0189      |\n",
      "|    value_loss           | 51.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61          |\n",
      "|    ep_rew_mean          | 61          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3373        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008001712 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x167511150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 4: Policy Learning (Using PPO from Stable Baselines3)\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6c624f-296d-47e7-8677-3f1bd4389532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 148.0\n",
      "Episode 2: Total Reward: 126.0\n",
      "Episode 3: Total Reward: 122.0\n",
      "Episode 4: Total Reward: 198.0\n",
      "Episode 5: Total Reward: 196.0\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Visualization\n",
    "def visualize_policy(env, model, num_episodes=5):\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs, _ = obs if isinstance(obs, tuple) else (obs, {})\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)[:4]\n",
    "            obs, _ = obs if isinstance(obs, tuple) else (obs, {})\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "visualize_policy(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cf854-6e14-44bc-8b91-278d432b2996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
